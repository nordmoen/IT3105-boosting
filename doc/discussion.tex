\section{Discussion}\label{discussion}
One important thing we found when examining the results were that the datasets 
are very important for the effect the boosting algorithm has. If a dataset has 
a low number of instances and/or a very heavily skewed proportion of 
instance-classes boosting, at least using our implementation of Naive Bayesian 
and Decision Tree Classifiers, is not very effective.

Another important factor for the effect of boosting is the depth of the 
decision trees the DTC is allowed to construct, if these are deep enough to 
allow a dataset to be split on all of its attributes it will construct 
over-fitted trees that only have a marginal improvement from boosting in the 
cases were the data is ambiguous or unknown.

One of the problems that we encountered when using boosting is that the weight
of a classifier would not reflect what it could actually explain. With this we
mean that a first classifier might get quite a lot correct and therefor get
a high weight, the next classifier will then only care about the instances that
the previous classifier got wrong and won't care about the rest. This might mean that
the second classifier were able to answer all the instances the first got wrong,
but miss classify the rest and thus get a low weight. This would manifest it self
when we try to classify the test set, the first classifier would suggest a wrong
answer, but because it has a high weight it will win over the second. Even though
that test instance might be one of the instances the first had gotten wrong.

This
seemed to be a problem for a couple of the other groups and one suggestion was to
partition the training set randomly using the weights, possibly with Fitness
proportionate selection\footnote{\url{
https://en.wikipedia.org/wiki/Roulette_wheel_selection}}. When generating a new
classifier the classifier would only see a subset of the actual training set, but
the weight of the classifier would not be effected by the set of training instances
which it did not see. This could mean that the classifiers would only get a weight
proportional to the actual explanation strength of that classifier. The intuition
behind this goes back to our description above, the first classifier might get a
lot correct, but when the second classifier tries to specialize on the instances
that the first got wrong it should not be punished for this. Unfortunately
we did not have time to try and implement this, but it could possibly solve some
of the problems that we experienced during boosting. There are however some
possible problems with this approach. For one thing giving each classifier only
a subset of the training data might mean that we would get very over fitted
classifiers which would go against what \adaboost{} tries to accomplish. Another
problem is the weighting, if we just gave each classifier a subset of the
training set, but kept the current way to give classifiers weight almost all
classifiers would get a much larger weight which would again give us a problem
during testing. This would most likely mean that a highly specialized classifier
would get a high weight, but this would also mean that it could "vote" for
solutions even though the current instance is outside the classifiers
specialization.

One observation that we can make is that for the naive Bayes classifier we can
see that the more classifiers we have the less is the standard deviation. This
is fully to be expected as we just have more classifiers which generally manages
to classify more of the cases and works towards the average.

One of the harder parts about this project was seeing if the booster worked even
though the results did not improve dramatically. We have checked several things,
but we are quite certain that our implementation is not very wrong. We might
have overlooked somethings, but the major part of the project should be
intelligent designed. One problem that we have observed is that we have a hard
time creating different trees in the DTC. This can easily be observed for some
of the datasets where even with five or ten classifiers we got the same average
with no deviation. The reason behind this is quite complex, since we do take
weights into account when creating the decision trees it should be effected, but
when the dataset has little deviation the trees are forced into this
configuration. If we grow the trees shallower we can see that the trees are
different which does lend some credence to our code. The problem though does
mean that boosting is affected because all the trees will "vote" for the same
thing overshadowing the other classifiers and skewing the results. This
observation also strengthen the theory that the dataset has a large involvement
in the outcome of our booster.

